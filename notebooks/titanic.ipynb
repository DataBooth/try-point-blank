{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating `pointblank` (Python version)\n",
    "\n",
    "[Pointblank documentation - Python](https://posit-dev.github.io/pointblank/)\n",
    "\n",
    "The following types of data tables are supported:\n",
    "\n",
    "- Polars DataFrame\n",
    "- Pandas DataFrame\n",
    "- DuckDB table\n",
    "- MySQL table\n",
    "- PostgreSQL table\n",
    "- SQLite table\n",
    "- Parquet\n",
    "\n",
    "Note that `pointblank` uses `Narwhals` to work with `Polars` and `Pandas` DataFrames and also integrates with `Ibis` to enable the use of `DuckDB`, `MySQL`, `PostgreSQL`, `SQLite`, `Parquet`, etc.\n",
    "\n",
    "So with `pointblank`, you can validate data in a variety of data sources and thus can handle quite large volumes of data efficiently e.g. via tools like `DuckDB`.\n",
    "\n",
    "## `pointblank` in R\n",
    "\n",
    "[Pointblank documentation - R](https://rstudio.github.io/pointblank/)\n",
    "\n",
    "The heritage of `pointblank` is in the R version initially released in 2017. The R version is more focused on the `dplyr` and `dbplyr` packages. The `pointblank` package in R is designed to work with `dplyr` and `dbplyr` data sources, and it can also be used with `dbplyr`-compatible databases.\n",
    "\n",
    "The Python version appears to be a relatively new project (late 2024).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import List, Set\n",
    "\n",
    "import duckdb\n",
    "import pandas as pd\n",
    "import pointblank as pb\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Titanic \n",
    "\n",
    "The Titianic dataset is a simple (and well-known) example with well-documented data quality issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TITANIC_URL = 'https://hbiostat.org/data/repo/titanic3.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "def get_create_titanic_db(db_path: str = 'titanic.duckdb', url: str = TITANIC_URL, force_create: bool = False) -> duckdb.DuckDBPyConnection:\n",
    "    \"\"\"\n",
    "    Connects to a DuckDB database, creating it if it doesn't exist,\n",
    "    and loads data from a CSV URL into a table named 'titanic'.  It enhances\n",
    "    the table by adding a unique 'id' column.\n",
    "\n",
    "    Args:\n",
    "        db_path (str, optional): The path to the DuckDB database file.\n",
    "            Defaults to 'titanic.duckdb'.\n",
    "        url (str, optional): The URL of the CSV file to load.\n",
    "            Defaults to TITANIC_URL.\n",
    "        force_create (bool, optional): If True, the database will be recreated\n",
    "            even if it already exists. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        duckdb.DuckDBPyConnection: A connection to the DuckDB database.\n",
    "    \"\"\"\n",
    "    if not Path(db_path).exists() or force_create:\n",
    "        # Connect to DuckDB and create a persistent database\n",
    "        con = duckdb.connect(database=db_path, read_only=False)\n",
    "\n",
    "        # Read the CSV files directly into DuckDB\n",
    "        con.execute(f\"CREATE TABLE IF NOT EXISTS titanic AS SELECT * FROM read_csv('{url}')\")\n",
    "        # Create a new table with a unique identifier\n",
    "        con.execute(\"\"\"\n",
    "            CREATE TABLE titanic_with_id AS \n",
    "            SELECT *, ROW_NUMBER() OVER () AS id \n",
    "            FROM titanic\n",
    "        \"\"\")\n",
    "        # Drop the original table and rename the new table\n",
    "        con.execute(\"DROP TABLE titanic\")\n",
    "        con.execute(\"ALTER TABLE titanic_with_id RENAME TO titanic\")\n",
    "    else:\n",
    "        # Connect to the existing database\n",
    "        con = duckdb.connect(database=db_path, read_only=False)\n",
    "    \n",
    "    return con\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data and create (or load) a database version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "con = get_create_titanic_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick checks on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con.sql(\"SELECT COUNT(*) AS n_record FROM titanic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at first 5 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "con.sql(\"SELECT * FROM titanic LIMIT 5\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "def create_random_samples(con: duckdb.DuckDBPyConnection, n_sample: int, RANDOM_SEED: int = 42) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Splits the 'titanic' dataset into a specified number of distinct, randomised samples.\n",
    "\n",
    "    This function aims to simulate a scenario where one sample of the data is used to define \n",
    "    data quality or validation rules, and the remaining samples are then evaluated\n",
    "    against these established rules. This approach allows for testing the robustness\n",
    "    and generalisability of the rules.\n",
    "\n",
    "    Args:\n",
    "        con (duckdb.DuckDBPyConnection): A connection to the DuckDB database containing the 'titanic' table.\n",
    "        n_sample (int): The number of distinct samples to create from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: A list of Pandas DataFrames, where each DataFrame represents a distinct,\n",
    "        randomized sample from the original 'titanic' dataset. The samples are designed to be \n",
    "        used for training and evaluating data quality/validation rules.\n",
    "    \"\"\"\n",
    "    total_rows = con.sql(\"SELECT COUNT(*) FROM titanic\").fetchone()[0]\n",
    "\n",
    "    # Calculate the number of rows per sample\n",
    "    rows_per_sample = math.ceil(total_rows / n_sample)\n",
    "\n",
    "    # Get all rows and shuffle them\n",
    "    all_rows = con.sql(\"SELECT * FROM titanic\").fetchdf()\n",
    "    shuffled_rows = all_rows.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "\n",
    "    samples = []\n",
    "    sample_size = []\n",
    "    for i in range(n_sample):\n",
    "        offset = i * rows_per_sample\n",
    "        sample = shuffled_rows.iloc[offset:offset + rows_per_sample]\n",
    "        sample_size.append(len(sample))\n",
    "        samples.append(sample)\n",
    "\n",
    "    return samples,sample_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SAMPLE = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples, sample_size = create_random_samples(con, N_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sizes of each sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the head of the first sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-check that the samples combine to match the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "# Fetch the original (entire) dataframe from the database\n",
    "original_df = con.sql(\"SELECT * FROM titanic\").df()\n",
    "\n",
    "# Concatenate all samples into a single dataframe\n",
    "concat_samples = pd.concat(samples).sort_values('id').reset_index(drop=True)\n",
    "\n",
    "# Check if the dataframes are identical\n",
    "identical = original_df.equals(concat_samples)\n",
    "\n",
    "print(f\"Original dataframe and concatenated samples identical? {'Yes' if identical else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pointblank functionality\n",
    "\n",
    "### `preview`\n",
    "\n",
    "Shows the head and tail of the data and stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb.preview(original_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data validation 'thresholds'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PASSENGER_CLASS: Set[int] = {1, 2, 3}       # Allowed values for passenger class\n",
    "PORT_EMBARKED: Set[str] = {\"C\", \"Q\", \"S\"}   # Allowed values for port of embarkation\n",
    "SURVIVED: Set[int] = {0, 1}                 # Allowed values for survival status\n",
    "SEX: Set[str] = {\"male\", \"female\"}          # Allowed values for sex\n",
    "MIN_AGE: int = 0                            # Minimum allowed age\n",
    "MAX_AGE: int = 70                           # Maximum allowed age\n",
    "MIN_FARE: int = 0                           # Minimum allowed fare\n",
    "MAX_FARE: int = 500                         # Maximum allowed fare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_titanic_data_sample(data: pd.DataFrame) -> pb.validate.Validate:\n",
    "    \"\"\"\n",
    "    Validates a Pandas DataFrame containing Titanic data using pointblank.\n",
    "\n",
    "    This function defines a set of validation rules for the Titanic dataset,\n",
    "    checking for missing values, value ranges, and allowed values in specific columns.\n",
    "\n",
    "    Args:\n",
    "        sample_df (pd.DataFrame): The Pandas DataFrame to validate.\n",
    "\n",
    "    Returns:\n",
    "        pb.ValidationSummary: A pointblank ValidationSummary object containing the validation results.\n",
    "    \"\"\"\n",
    "\n",
    "    validation = (\n",
    "        pb.Validate(data=data, label=\"Titanic Data Validation\")\n",
    "        .col_vals_not_null(\"survived\")\n",
    "        .col_vals_not_null(\"pclass\")\n",
    "        .col_vals_not_null(\"sex\")\n",
    "        .col_vals_not_null(\"age\")\n",
    "        .col_vals_not_null(\"ticket\")\n",
    "        .col_vals_not_null(\"fare\")\n",
    "        .col_vals_not_null(\"embarked\")\n",
    "        .col_vals_between(\"age\", MIN_AGE, MAX_AGE, na_pass=True)\n",
    "        .col_vals_between(\"fare\", MIN_FARE, MAX_FARE)\n",
    "        .col_vals_in_set(\"pclass\", PASSENGER_CLASS)\n",
    "        .col_vals_in_set(\"embarked\", PORT_EMBARKED)\n",
    "        .col_vals_in_set(\"survived\", SURVIVED)\n",
    "        .col_vals_in_set(\"sex\", SEX)\n",
    "        .interrogate()\n",
    "    )\n",
    "\n",
    "    return validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = validate_titanic_data_sample(original_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.get_tabular_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation.get_sundered_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Use the first sample to \"define\" the thresholds and then process the remaining samples with thresholds and actions.\n",
    "\n",
    "https://posit-dev.github.io/pointblank/user-guide/thresholds.html\n",
    "\n",
    "https://posit-dev.github.io/pointblank/user-guide/actions.html\n",
    "\n",
    "TODO: Also take a look at working directly with DuckDB data table not via Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate the individual samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "\n",
    "for i, sample in enumerate(samples):\n",
    "    validation = validate_titanic_data_sample(sample)\n",
    "    display(Markdown(f\"**Sample {i+1}**: {len(sample)} rows\"))\n",
    "    display(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
